{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q1) Explain the properties of the F-distribution. \n",
    "Answer-\n",
    "    The F-distribution is a probability distribution that arises when comparing two sample variances, commonly used in ANOVA \n",
    "    (Analysis of Variance) and regression analysis. Here are its key properties:\n",
    "    1. Non-negativity: The F-distribution only takes non-negative values (i.e.,F ≥ 0) because it is based on the ratio of two squared values\n",
    "       (variances), which cannot be negative.\n",
    "\n",
    "    2. Shape: The shape of the F-distribution is asymmetric and right-skewed, especially when the sample sizes are small. As the sample \n",
    "       sizes increase, the distribution becomes more symmetric.\n",
    "\n",
    "    3. Degrees of Freedom: The F-distribution is defined by two degrees of freedom:\n",
    "       d1 (numerator degrees of freedom): associated with the variance in the numerator.\n",
    "       d2 (denominator degrees of freedom): associated with the variance in the denominator.\n",
    "    \n",
    "    4. Mean of the F-Distribution: The mean of an F-distribution exists and is equal to d2 / d2 - 2 if d2 > 2.\n",
    "\n",
    "    5. Right-Tailed Test: The F-distribution is commonly used in right-tailed hypothesis tests, meaning we often compare observed F-values \n",
    "       to critical values on the right side of the distribution.\n",
    "\n",
    "    6. Use in Variance Testing: The F-distribution is particularly useful when comparing variances between groups. It's widely used in ANOVA\n",
    "       to test whether multiple group means are equal and in F-tests to determine if two variances are significantly different.\n",
    "\n",
    "    7. Additivity of Variances: When used in ANOVA, the F-distribution allows for the decomposition of total variance into between-group \n",
    "       variance and within-group variance.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q2) In which types of statistical tests is the F-distribution used, and why is it appropriate for these tests?\n",
    "Answer-\n",
    "    The F-distribution is commonly used in statistical tests that involve comparing variances or assessing relationships between group means,\n",
    "    particularly in the following:\n",
    "\n",
    "    1. Analysis of Variance (ANOVA):\n",
    "       * Purpose: ANOVA tests are used to determine if there are significant differences between the means of three or more groups.\n",
    "       * Why F-distribution is appropriate: In ANOVA, the F-test compares the variance between groups (which would suggest a difference in \n",
    "         means) to the variance within groups (expected variance if there were no difference). The F-distribution allows us to understand if\n",
    "         observed differences are likely due to random chance or if they are statistically significant.\n",
    "    2. Regression Analysis (F-test for overall significance):\n",
    "       * Purpose: In regression, the F-test assesses if the overall model (multiple predictors together) significantly predicts the dependent\n",
    "         variable.\n",
    "       * Why F-distribution is appropriate: The F-test in regression examines the ratio of explained variance (model) to unexplained variance\n",
    "         (residuals). The F-distribution helps determine if the observed relationship is stronger than what might be expected by chance, \n",
    "         implying that the model has predictive value.\n",
    "    3. Comparing Two Variances:\n",
    "       * Purpose: Sometimes, we need to compare the variances of two independent groups to determine if they come from populations with equal\n",
    "         variances.\n",
    "       * Why F-distribution is appropriate: Since the F-distribution describes the ratio of two variances, it is naturally suited for \n",
    "         determining whether the variance in one sample is significantly different from the variance in another.\n",
    "    \n",
    "        The F-distribution is well-suited to these tests because it reflects the expected distribution of variance ratios under the null \n",
    "        hypothesis, allowing for the comparison of sample variance estimates relative to expected random variability.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q3) What are the key assumptions required for conducting an F-test to compare the variances of two populations?\n",
    "Answer-\n",
    "    The F-test for comparing the variances of two populations relies on a few key assumptions:\n",
    "\n",
    "    1. Independence of Samples:\n",
    "    The two samples being compared should be independent of each other. This means that the selection of one sample should not influence the\n",
    "    selection of the other.\n",
    "    \n",
    "    2. Normality of Distributions:\n",
    "    Both populations from which the samples are drawn should follow a normal distribution. The F-test is sensitive to departures from \n",
    "    normality, especially if sample sizes are small.\n",
    "    \n",
    "    3. Random Sampling:\n",
    "    Each sample should be randomly selected from its respective population to ensure that it represents the population accurately and \n",
    "    minimizes bias.\n",
    "    \n",
    "    4. Ratio of Variances:\n",
    "    The F-test is appropriate for testing the null hypothesis that the variances of two populations are equal. It should not be used if we're\n",
    "    interested in testing other characteristics, such as means.\n",
    "    \n",
    "    Why These Assumptions Matter:\n",
    "    Normality: The F-distribution, which forms the basis of the F-test, assumes normally distributed populations. Deviations from normality \n",
    "    can lead to inaccurate results.\n",
    "    Independence and Random Sampling: Violating these can introduce dependencies and biases that distort the F-ratio, potentially leading to\n",
    "    incorrect conclusions about the equality of variances.\n",
    "    \n",
    "    If these assumptions are met, the F-test can effectively determine whether there is a significant difference in the variances of the two \n",
    "    populations.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q4) What is the purpose of ANOVA, and how does it differ from a t-test? \n",
    "Answer-\n",
    "    The purpose of ANOVA (Analysis of Variance) is to determine whether there are statistically significant differences between the means of\n",
    "    three or more groups. It evaluates the variability within each group and compares it to the variability between groups to see if the\n",
    "    observed differences in group means are likely to be due to chance or if they reflect true differences in the populations.\n",
    "\n",
    "    Key Differences Between ANOVA and a t-Test:\n",
    "    1. Number of Groups:\n",
    "       * t-Test: Typically compares the means of two groups.\n",
    "       * ANOVA: Used when comparing the means of three or more groups.\n",
    "    \n",
    "    2. Type of Variability Compared:\n",
    "       * t-Test: Assesses whether the difference in means between two groups is significant relative to the variability within each group.\n",
    "       * ANOVA: Compares the variance between multiple group means to the variance within the groups.\n",
    "    \n",
    "    3. Multiple Comparisons:\n",
    "       * t-Test: For comparing more than two groups, multiple t-tests would need to be performed, which increases the risk of a Type I error\n",
    "         (false positive).\n",
    "       * ANOVA: Can handle comparisons across multiple groups simultaneously, reducing the need for multiple tests and controlling the \n",
    "        overall error rate.\n",
    "    \n",
    "    4. Hypotheses Tested:\n",
    "       * t-Test: Tests the null hypothesis that the means of the two groups are equal.\n",
    "       * ANOVA: Tests the null hypothesis that all group means are equal. If ANOVA finds a significant result, it indicates that at least one\n",
    "         group mean is different from the others but does not specify which groups differ. (Post-hoc tests are required for specific group \n",
    "         comparisons.)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q5)  Explain when and why you would use a one-way ANOVA instead of multiple t-tests when comparing more than two groups. \n",
    "Answer-\n",
    "    When comparing more than two groups, a one-way ANOVA is preferred over multiple t-tests because:\n",
    "    1. Control Over Type I Error:\n",
    "       * Every time you conduct a t-test, there is a risk of a Type I error (false positive). If you perform multiple t-tests, this risk\n",
    "         accumulates, increasing the likelihood of incorrectly finding a significant result.\n",
    "       * One-way ANOVA controls for this by assessing all group comparisons in a single test, keeping the overall Type I error rate at the\n",
    "         desired significance level (e.g., 0.05).\n",
    "    2. Efficiency and Simplicity:\n",
    "       * Running multiple t-tests can be time-consuming and complex, especially as the number of groups increases. For example, comparing \n",
    "         four groups would require six pairwise t-tests.\n",
    "       * A one-way ANOVA provides a single, efficient analysis to determine if there is any difference among the group means, without needing\n",
    "         separate tests for each pair.\n",
    "    3. Interpretation and Insight:\n",
    "       * One-way ANOVA evaluates the overall variance among all groups, answering the question, \"Is there at least one significant difference\n",
    "         among these groups?\" without requiring pairwise comparisons.\n",
    "       * If ANOVA finds a significant result, post-hoc tests (like Tukey's HSD) can then be used to identify specifically which groups \n",
    "         differ, making the process more systematic.\n",
    "\n",
    "    Example Scenario:\n",
    "    Suppose you are studying the effects of three different diets on weight loss. Instead of performing multiple t-tests to compare each diet\n",
    "    against the others, a one-way ANOVA lets you test whether any of the three diets produces a statistically different effect on weight loss,\n",
    "    with only one analysis, keeping your error rate controlled and your analysis straightforward.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q6)  Explain how variance is partitioned in ANOVA into between-group variance and within-group variance. How does this partitioning \n",
    "     contribute to the calculation of the F-statistic?\n",
    "Answer-\n",
    "    In ANOVA, variance is partitioned into two main components:\n",
    "    1. Between-Group Variance: This measures the variability among the group means themselves. It reflects differences between groups, which\n",
    "       might indicate the effect of the independent variable (e.g., different treatments or conditions) on the outcome.\n",
    "\n",
    "    2. Within-Group Variance: This measures the variability within each group, reflecting differences among individual observations within \n",
    "       the same group. This variation is often attributed to random error or individual differences rather than the independent variable.\n",
    "\n",
    "    Partitioning of Variance in ANOVA:\n",
    "    * Total Variance (SST): This is the total variability in the data, calculated by summing the squared differences between each observation\n",
    "      and the overall mean.\n",
    "    * Between-Group Variance (SSB): This is the variability between the groups, calculated by summing the squared differences between each \n",
    "      group mean and the overall mean, weighted by the group size.\n",
    "    * Within-Group Variance (SSW): This is the variability within the groups, calculated by summing the squared differences between each \n",
    "      observation and its group mean.\n",
    "    \n",
    "    These sums of squares (SST, SSB, SSW) represent the total variance, partitioned into between-group and within-group variance:\n",
    "    SST = SSB + SSW\n",
    "\n",
    "    Calculation of the F-Statistic:\n",
    "    The F-statistic is calculated as a ratio of the between-group variance to the within-group variance:\n",
    "    1. Mean Square Between (MSB): Divide SSB by the degrees of freedom for between-group, which is the number of groups minus 1.\n",
    "\n",
    "    2. Mean Square Within (MSW): Divide SSW by the degrees of freedom for within-group, which is the total number of observations minus the\n",
    "       number of groups.\n",
    "    \n",
    "    3. F-Statistic: Finally, divide MSB by MSW.\n",
    "    \n",
    "    Interpretation:\n",
    "    * A larger F-statistic suggests that between-group variance is substantially greater than within-group variance, implying that the group\n",
    "      means differ significantly.\n",
    "    * If the F-statistic is high enough (based on an F-distribution critical value), we reject the null hypothesis, concluding that at least\n",
    "      one group mean is significantly different from the others.\n",
    "    \n",
    "    This partitioning allows ANOVA to isolate the effect of the independent variable (between-group variance) from random error \n",
    "    (within-group variance), providing a clear test of whether group differences are statistically significant.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q7) Compare the classical (frequentist) approach to ANOVA with the Bayesian approach. What are the key differences in terms of how they \n",
    "    handle uncertainty, parameter estimation, and hypothesis testing?\n",
    "Answer-\n",
    "    The classical (frequentist) and Bayesian approaches to ANOVA differ in their fundamental philosophies, especially in handling uncertainty,\n",
    "    parameter estimation, and hypothesis testing. Here’s a breakdown of their key differences:\n",
    "    1. Handling Uncertainty - \n",
    "       * Frequentist ANOVA: Assumes that data is drawn from fixed distributions with unknown parameters. Uncertainty is based on long-run \n",
    "         frequencies, meaning conclusions about group differences are made based on the probability of observing data under the null hypothesis.\n",
    "       * Bayesian ANOVA: Treats parameters (like group means and variances) as random variables with their own probability distributions. \n",
    "         Uncertainty is directly modeled through prior distributions, which are updated with observed data to produce posterior distributions.\n",
    "         This approach allows for direct statements about the probability of parameter values given the data.\n",
    "    \n",
    "    2. Parameter Estimation -\n",
    "       * Frequentist ANOVA: Estimates parameters (e.g., group means, variances) using methods like maximum likelihood, relying on sample data\n",
    "         alone without any prior assumptions about the parameters. Parameter estimates are point estimates and do not convey probability about\n",
    "         their values.\n",
    "       * Bayesian ANOVA: Combines prior beliefs about parameters with observed data to generate posterior distributions. These posteriors \n",
    "         provide a range of plausible values for parameters, incorporating both prior knowledge and sample evidence, allowing uncertainty to \n",
    "         be quantified directly.\n",
    "    \n",
    "    3. Hypothesis Testing -\n",
    "       * Frequentist ANOVA: Uses an F-test to compare variances and assess if there are significant differences between group means. It \n",
    "         operates by testing the null hypothesis (no difference between group means) against an alternative hypothesis, giving a p-value to\n",
    "         assess statistical significance. This approach is all-or-nothing, as we either reject or fail to reject the null hypothesis based on\n",
    "         a significance threshold (e.g., 0.05).\n",
    "       * Bayesian ANOVA: Does not rely on p-values or reject/fail-to-reject decisions. Instead, it calculates the posterior probability of \n",
    "         different hypotheses and compares them. For example, a Bayesian approach can directly calculate the probability that a particular \n",
    "         group's mean is greater than another's, based on the data. This flexibility allows for richer interpretations, such as quantifying \n",
    "         evidence in favor of one hypothesis over another without strict rejection rules.\n",
    "    \n",
    "    4. Interpretation of Results -\n",
    "       * Frequentist ANOVA: Results are interpreted with respect to sampling distributions; conclusions are about the likelihood of observing\n",
    "         data given the null hypothesis is true, rather than about specific parameter values.\n",
    "       * Bayesian ANOVA: Conclusions are framed in terms of posterior probabilities. This makes it easier to interpret results as the \n",
    "         probability of hypotheses or parameter values directly, based on observed data and prior knowledge.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance of Profession A: 32.8\n",
      "Variance of Profession B: 15.7\n",
      "F-statistic: 2.089171974522293\n",
      "p-value: 0.49304859900533904\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Q8) You have two sets of data representing the incomes of two different professions\n",
    "    Profession A: [48, 52, 55, 60, 62'\n",
    "    Profession B: [45, 50, 55, 52, 47] Perform an F-test to determine if the variances of the two professions' incomes are equal.\n",
    "    What are your conclusions based on the F-test?\n",
    "    Task: Use Python to calculate the F-statistic and p-value for the given data.\n",
    "    Objective: Gain experience in performing F-tests and interpreting the results in terms of variance comparison.\n",
    "Answer- \n",
    "\"\"\"\n",
    "import numpy as np\n",
    "from scipy.stats import f\n",
    "\n",
    "profession_A = [48, 52, 55, 60, 62]\n",
    "profession_B = [45, 50, 55, 52, 47]\n",
    "\n",
    "var_A = np.var(profession_A, ddof=1)  # Sample variance for Profession A\n",
    "var_B = np.var(profession_B, ddof=1)  # Sample variance for Profession B\n",
    "\n",
    "# Calculate the F-statistic\n",
    "if var_A > var_B:\n",
    "    F_statistic = var_A / var_B\n",
    "    dfn, dfd = len(profession_A) - 1, len(profession_B) - 1  # Degrees of freedom for A and B\n",
    "else:\n",
    "    F_statistic = var_B / var_A\n",
    "    dfn, dfd = len(profession_B) - 1, len(profession_A) - 1\n",
    "\n",
    "# Calculate the p-value for the F-test\n",
    "p_value = 2 * (1 - f.cdf(F_statistic, dfn, dfd))  # Two-tailed p-value\n",
    "\n",
    "print(f\"Variance of Profession A: {var_A}\")\n",
    "print(f\"Variance of Profession B: {var_B}\")\n",
    "print(f\"F-statistic: {F_statistic}\")\n",
    "print(f\"p-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-statistic: 67.87330316742101\n",
      "p-value: 2.8706641879370266e-07\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Q9) Conduct a one-way ANOVA to test whether there are any statistically significant differences in average heights between three different \n",
    "    regions with the following data:\n",
    "    Region A: [160, 162, 165, 158, 164]\n",
    "    Region B: [172, 175, 170, 168, 174]\n",
    "    Region C: [180, 182, 179, 185, 183]\n",
    "    Task: Write Python code to perform the one-way ANOVA and interpret the results\n",
    "    Objective: Learn how to perform one-way ANOVA using Python and interpret F-statistic and p-value.\n",
    "Answer-\n",
    "\"\"\"\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "region_A = [160, 162, 165, 158, 164]\n",
    "region_B = [172, 175, 170, 168, 174]\n",
    "region_C = [180, 182, 179, 185, 183]\n",
    "\n",
    "F_statistic, p_value = f_oneway(region_A, region_B, region_C)\n",
    "\n",
    "print(f\"F-statistic: {F_statistic}\")\n",
    "print(f\"p-value: {p_value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
